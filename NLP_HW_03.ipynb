{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP HW 03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMlnelPakGrLZjXqflFYJWw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhilkoditala/CSC-8980-NLP/blob/main/NLP_HW_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1jOE1rcgHkG"
      },
      "source": [
        "# Name: Nikhil Koditala\n",
        "\n",
        "# Panther ID: 002571023"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Tef5I3G6hWAA",
        "outputId": "49295c14-090b-4fce-86b3-79ffab4775dc"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "nltk.download('vader_lexicon')\n",
        "from heapq import nlargest\n",
        "import random, copy\n",
        "import sklearn.metrics\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ipwXsWhhsvz"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFlKvM_fhY6u"
      },
      "source": [
        "# unzip data\n",
        "!tar -xvf  '/content/review_polarity.tar.gz' -C '/content/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URLyAzLCijfI"
      },
      "source": [
        "positive_file_paths = os.listdir('/content/txt_sentoken/pos')\n",
        "negative_file_paths = os.listdir('/content/txt_sentoken/neg') "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kIIPPqYikQT"
      },
      "source": [
        "pos_text = []\n",
        "for path in positive_file_paths:\n",
        "  f = open(os.path.join('/content/txt_sentoken/pos',path),'r')\n",
        "  pos_text.append(f.read())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sBIRdswjqrD"
      },
      "source": [
        "neg_text = []\n",
        "for path in negative_file_paths:\n",
        "  f = open(os.path.join('/content/txt_sentoken/neg',path),'r')\n",
        "  neg_text.append(f.read())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hZOKu8rh22_"
      },
      "source": [
        "1 Using NLTK tokenize all documents, separated by polarity, ​remove stop words​, and list the top 20 most frequent tokens (and their counts) for the positive reviews, and the top 20 most frequent tokens (and their counts). What kind of things do you notice are different between the two sets? (30 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvXAs8f4q4aX"
      },
      "source": [
        "stop_words = list(stopwords.words('english')) "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbCFHFSsh2PD"
      },
      "source": [
        "#pos\n",
        "pos_tokens = []\n",
        "for pos_file in pos_text:\n",
        "  pos_tokens.extend(nltk.word_tokenize(pos_file))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyv_I45arNjk"
      },
      "source": [
        "pos_tokens_after_stop_words = []\n",
        "\n",
        "for token in pos_tokens:\n",
        "  if(token not in stop_words):\n",
        "    pos_tokens_after_stop_words.append(token)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3BtpjHZtlCE"
      },
      "source": [
        "pos_frequency_dict = {}\n",
        "\n",
        "for token in pos_tokens_after_stop_words:\n",
        "  if(token in pos_frequency_dict):\n",
        "    pos_frequency_dict[token] += 1\n",
        "  else:\n",
        "    pos_frequency_dict[token] = 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5Sc4FdmuyHOM",
        "outputId": "5fa4d106-e0bd-49cf-f4b9-2abaedfee5c2"
      },
      "source": [
        "pos_ten_most_frequent_tokens = nlargest(20, pos_frequency_dict, key=pos_frequency_dict.get)\n",
        "\n",
        "for token in pos_ten_most_frequent_tokens:\n",
        "  print(token,' - ',pos_frequency_dict[token])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ",  -  42448\n",
            ".  -  33714\n",
            "'s  -  9473\n",
            "``  -  8494\n",
            ")  -  6039\n",
            "(  -  6014\n",
            "film  -  5186\n",
            "one  -  2943\n",
            "n't  -  2775\n",
            "movie  -  2497\n",
            "like  -  1713\n",
            "?  -  1570\n",
            ":  -  1502\n",
            "story  -  1231\n",
            "also  -  1200\n",
            "good  -  1190\n",
            "even  -  1175\n",
            "time  -  1171\n",
            "would  -  1079\n",
            "character  -  1067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKgxZwTKhmfC"
      },
      "source": [
        "#neg\n",
        "neg_tokens = []\n",
        "for neg_file in neg_text:\n",
        "  neg_tokens.extend(nltk.word_tokenize(neg_file))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJJweeUytklN"
      },
      "source": [
        "neg_tokens_after_stop_words = []\n",
        "\n",
        "for token in neg_tokens:\n",
        "  if(token not in stop_words):\n",
        "    neg_tokens_after_stop_words.append(token)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeYMTzB5wxmd"
      },
      "source": [
        "neg_frequency_dict = {}\n",
        "\n",
        "for token in neg_tokens_after_stop_words:\n",
        "  if(token in neg_frequency_dict):\n",
        "    neg_frequency_dict[token] += 1\n",
        "  else:\n",
        "    neg_frequency_dict[token] = 1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lQ87dfzkzJ8j",
        "outputId": "9ea89c95-b21c-4993-943e-817bad951e8e"
      },
      "source": [
        "neg_ten_most_frequent_tokens = nlargest(20, neg_frequency_dict, key=neg_frequency_dict.get)\n",
        "\n",
        "for token in neg_ten_most_frequent_tokens:\n",
        "  print(token,' - ',neg_frequency_dict[token])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ",  -  35269\n",
            ".  -  32162\n",
            "``  -  9123\n",
            "'s  -  8655\n",
            ")  -  5742\n",
            "(  -  5650\n",
            "film  -  4257\n",
            "n't  -  3442\n",
            "movie  -  3174\n",
            "one  -  2637\n",
            "?  -  2201\n",
            "like  -  1832\n",
            ":  -  1540\n",
            "even  -  1381\n",
            "would  -  1185\n",
            "good  -  1126\n",
            "time  -  1111\n",
            "!  -  1056\n",
            "get  -  1039\n",
            "bad  -  1019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LaGSqHOdAhK"
      },
      "source": [
        "**What kind of things do you notice are different between the two sets?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BXNbhmXc9VH"
      },
      "source": [
        "Tokens from positive documents have a lot of words with positive sentiments such as good. Whereas tokens from negative sentences contains words such as bad which are of negative sentiement.  \n",
        "\n",
        "\n",
        "There are also few tokens which are frequently used in both positive and negative documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsxfSYosh4hz"
      },
      "source": [
        "2 Using the code from previous lectures, build 3 polarity classifiers using the following parameters (20 points). Note: just train the models.\n",
        "\n",
        "a) For training: use 50% of the positive dataset and 70% of the negative dataset. For your model use: NaiveBayes with the TF-IDF vectorizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkZqFYwadmgk"
      },
      "source": [
        "def generate_splits(pos_percentage, neg_percentage):\n",
        "  pos_text_r = copy.deepcopy(pos_text)\n",
        "  random.seed(123)\n",
        "  random.shuffle(pos_text_r)\n",
        "\n",
        "  neg_text_r = copy.deepcopy(neg_text)\n",
        "  random.shuffle(neg_text_r)\n",
        "\n",
        "  pos_index = int((len(pos_text_r)/100)*pos_percentage)\n",
        "  neg_index = int((len(neg_text_r)/100)*neg_percentage)\n",
        "\n",
        "  return pos_text_r[:pos_index], pos_text_r[pos_index:], neg_text_r[:neg_index], neg_text_r[neg_index:]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaSw5kgbh62r"
      },
      "source": [
        "pos_train_a, pos_test_a, neg_train_a, neg_test_a = generate_splits(50,70)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6Qwndw6uADa"
      },
      "source": [
        "train_x_a = pos_train_a + neg_train_a\n",
        "train_y_a = [1]*len(pos_train_a) + [0]*len(neg_train_a)\n",
        "\n",
        "c = list(zip(train_x_a, train_y_a))\n",
        "random.shuffle(c)\n",
        "train_x_a, train_y_a = zip(*c)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wwYCwLb3n_lP",
        "outputId": "70780dab-bc57-4980-8117-e14bd507f4a5"
      },
      "source": [
        "model_a = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "model_a.fit(train_x_a, train_y_a)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('multinomialnb',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUYrST9D_waS"
      },
      "source": [
        "b) For training: use 50% of the negative dataset and 70% of the positive dataset.For your model use: NaiveBayes with the TF-IDF vectorizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms-svJ2F_xsh"
      },
      "source": [
        "pos_train_b, pos_test_b, neg_train_b, neg_test_b = generate_splits(70,50)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CwEkmsTvd3B"
      },
      "source": [
        "train_x_b = pos_train_b + neg_train_b\n",
        "train_y_b = [1]*len(pos_train_b) + [0]*len(neg_train_b)\n",
        "\n",
        "c = list(zip(train_x_b, train_y_b))\n",
        "random.shuffle(c)\n",
        "train_x_b, train_y_b = zip(*c)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SpQlAoC9hRBq",
        "outputId": "b687e28e-6f97-408b-b653-cd78d4175e14"
      },
      "source": [
        "model_b = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "model_b.fit(train_x_b, train_y_b)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('multinomialnb',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDBo2n41_yLz"
      },
      "source": [
        "c) For training: use 25% of the negative dataset and 25% of the positive dataset. For your model use: SVM with the TF-IDF vectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsEb-1ZE_ytF"
      },
      "source": [
        "pos_train_c, pos_test_c, neg_train_c, neg_test_c = generate_splits(25,25)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgsMCZY3vpZN"
      },
      "source": [
        "train_x_c = pos_train_c + neg_train_c\n",
        "train_y_c = [1]*len(pos_train_c) + [0]*len(neg_train_c)\n",
        "\n",
        "c = list(zip(train_x_c, train_y_c))\n",
        "random.shuffle(c)\n",
        "train_x_c, train_y_c = zip(*c)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "k_61ZREXnH1e",
        "outputId": "db7bd164-207f-45ca-c45f-1c5798908e6c"
      },
      "source": [
        "model_c = make_pipeline(TfidfVectorizer(), SVC())\n",
        "model_c.fit(train_x_c,train_y_c)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('svc',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqv_Lmc1_1na"
      },
      "source": [
        "3 Using the models from question 2, evaluate them on their individual rest of the dataset. This is, for a) 50% positive and 30% negative, for b) 50% negative and 30% positive, and for c) 75% negative and 75% positive. Calculate and show ONLY the following metrics for each model: Accuracy, Precision, Recall, Macro F1-score. (15 points)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCW-BNM3oOoN"
      },
      "source": [
        "y_test_model_a = [1]*len(pos_test_a) + [0]*len(neg_test_a)\n",
        "y_pred_model_a = model_a.predict(pos_test_a + neg_test_a)\n",
        "model_a_accuracy = sklearn.metrics.accuracy_score(y_pred_model_a,y_test_model_a)\n",
        "model_a_recall = sklearn.metrics.recall_score(y_test_model_a,y_pred_model_a)\n",
        "model_a_percision =  sklearn.metrics.precision_score(y_test_model_a,y_pred_model_a)\n",
        "model_a_f1score =  sklearn.metrics.f1_score(y_test_model_a,y_pred_model_a,average='macro')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hF-zFTMsd5w"
      },
      "source": [
        "y_test_model_b = [1]*len(pos_test_b) + [0]*len(neg_test_b)\n",
        "y_pred_model_b = model_b.predict(pos_test_b + neg_test_b)\n",
        "model_b_accuracy = sklearn.metrics.accuracy_score(y_pred_model_b,y_test_model_b)\n",
        "model_b_recall = sklearn.metrics.recall_score(y_test_model_b,y_pred_model_b)\n",
        "model_b_percision =  sklearn.metrics.precision_score(y_test_model_b,y_pred_model_b)\n",
        "model_b_f1score =  sklearn.metrics.f1_score(y_test_model_b,y_pred_model_b,average='macro')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64V__KuasqX8"
      },
      "source": [
        "y_test_model_c = [1]*len(pos_test_c) + [0]*len(neg_test_c)\n",
        "y_pred_model_c = model_c.predict(pos_test_c + neg_test_c)\n",
        "model_c_accuracy = sklearn.metrics.accuracy_score(y_pred_model_c,y_test_model_c)\n",
        "model_c_recall = sklearn.metrics.recall_score(y_test_model_c,y_pred_model_c)\n",
        "model_c_percision =  sklearn.metrics.precision_score(y_test_model_c,y_pred_model_c)\n",
        "model_c_f1score =  sklearn.metrics.f1_score(y_test_model_c,y_pred_model_c,average='macro')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "PIXvyKoro3iX",
        "outputId": "d5b09c39-b1b0-4a7d-f5d4-e9af25d8877d"
      },
      "source": [
        "pd.DataFrame(np.array([['Model A',50,70,model_a_accuracy,model_a_recall,model_a_percision,model_a_f1score],\n",
        "                             ['Model B',70,50,model_b_accuracy,model_b_recall,model_b_percision,model_b_f1score],\n",
        "                             ['Model C',25,25,model_c_accuracy,model_c_recall,model_c_percision,model_c_f1score],\n",
        "]),columns=['Model Name','Positive Percentage','Negative Percentage','Accuracy', 'Recall', 'Precision','F1-Score'])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model Name</th>\n",
              "      <th>Positive Percentage</th>\n",
              "      <th>Negative Percentage</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Precision</th>\n",
              "      <th>F1-Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Model A</td>\n",
              "      <td>50</td>\n",
              "      <td>70</td>\n",
              "      <td>0.37625</td>\n",
              "      <td>0.002</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2749714401951329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Model B</td>\n",
              "      <td>70</td>\n",
              "      <td>50</td>\n",
              "      <td>0.375</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.2727272727272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Model C</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>0.7526666666666667</td>\n",
              "      <td>0.7786666666666666</td>\n",
              "      <td>0.7401774397972116</td>\n",
              "      <td>0.7524993562314792</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Model Name Positive Percentage  ...           Precision            F1-Score\n",
              "0    Model A                  50  ...                 1.0  0.2749714401951329\n",
              "1    Model B                  70  ...               0.375  0.2727272727272727\n",
              "2    Model C                  25  ...  0.7401774397972116  0.7524993562314792\n",
              "\n",
              "[3 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnaP1ACY_98S"
      },
      "source": [
        "**4) Using the model performance metrics from question 3, answer the following questions. Please provide logical and intuitive rationale for your answers, simple answers like: because it has the best score, will not be sufficient. (40 points):**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg0BwxB7wKW1"
      },
      "source": [
        "**a) What is the best performing model?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVR1StUKwKT1"
      },
      "source": [
        "SVM with TFIDF is the best performing model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG4IAQLqwKRj"
      },
      "source": [
        "**b) Why do you think this is the best performing model?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBP2KIUKwJ2l"
      },
      "source": [
        "SVM performed best because the training set is balanced. For model 1 and model 2 the training set is highly imbalanced which might have affected model's performance. But for Model C the training set is balanced and it led to better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Us6DD6GwON8"
      },
      "source": [
        "**c) How does class imbalance play in determining polarity?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIc2-xsswOHS"
      },
      "source": [
        "By looking at the above metrics table, it clearly shows that model A and model B are affected by class imbalances. For model A, the percision is 1 and recall is 0.002. This clearly shows that the model is predicting 0 for all the values. This is because the training dataset for Model A is highly unbalanced with negative labels. \n",
        "\n",
        "Similarly, for model B, the recall is 1 and precision is 0.375. As model B is inbalanced with large number of positive the model is predicting 1 for all the values.\n",
        "\n",
        "Due to the imbalance in the dataset, the models A and B have given importance to positive and negative labels respectively. Whereas Model C, being balanced dataset, performed better than the above two.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-erGKCwwOAc"
      },
      "source": [
        "**d) Do you think either more data or a better model is a better approach for this kind of task?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "153gan4-bd_s"
      },
      "source": [
        "Depeding on the model, the performance of our classification task may improve. If we have large data, by using deep neural networks we might be able to build a better performing model.\n",
        "\n",
        "But, having more data doesn't always mean that the performance of the model will improve as it might lead to overfitting of the model. So we have to choose the model and dataset optimally such that it doesn't lead to overfitting or underfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tcwFgCsADoe"
      },
      "source": [
        "**5) Using NLTK and VADER, calculate the sentiment score for all documents in the positive polarity. Calculate the polarity threshold needed (and reasonable) to have the majority of the document labels match. Do the same for the negative class. Provide the threshold needed, the reason why you think this threshold is reasonable, and the accuracy percentage (how many documents are correctly labeled using this threshold). (45 points):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx3yt--obpeg"
      },
      "source": [
        "sia = SIA()"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIA-MxS5cJ3H"
      },
      "source": [
        "positive_sentiments = pd.DataFrame()"
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGlIjOI_cJxP"
      },
      "source": [
        "for path,doc in zip(positive_file_paths,pos_text):\n",
        "  temp = sia.polarity_scores(doc)\n",
        "  positive_sentiments = positive_sentiments.append([[path,temp['compound'],temp['neg'],temp['neu'],temp['pos']]])\n"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-UuVlUVA582"
      },
      "source": [
        "positive_sentiments.columns = ['Filename','Compound','Negative','Neutral','Positive']"
      ],
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "Z2vWRhWDAtqj",
        "outputId": "6f6d620e-e5a4-48b1-dd06-55b63ceaa245"
      },
      "source": [
        "positive_sentiments"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filename</th>\n",
              "      <th>Compound</th>\n",
              "      <th>Negative</th>\n",
              "      <th>Neutral</th>\n",
              "      <th>Positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv815_22456.txt</td>\n",
              "      <td>0.9931</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.736</td>\n",
              "      <td>0.160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv274_25253.txt</td>\n",
              "      <td>0.9996</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.713</td>\n",
              "      <td>0.204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv927_10681.txt</td>\n",
              "      <td>0.9210</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.852</td>\n",
              "      <td>0.079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv442_13846.txt</td>\n",
              "      <td>-0.9993</td>\n",
              "      <td>0.199</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv551_10565.txt</td>\n",
              "      <td>0.9973</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.753</td>\n",
              "      <td>0.160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv117_24295.txt</td>\n",
              "      <td>-0.9568</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.674</td>\n",
              "      <td>0.157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv602_8300.txt</td>\n",
              "      <td>-0.9861</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.848</td>\n",
              "      <td>0.052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv560_17175.txt</td>\n",
              "      <td>0.9960</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.759</td>\n",
              "      <td>0.134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv891_6385.txt</td>\n",
              "      <td>-0.9181</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.771</td>\n",
              "      <td>0.109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv677_17715.txt</td>\n",
              "      <td>0.9880</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.772</td>\n",
              "      <td>0.127</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Filename  Compound  Negative  Neutral  Positive\n",
              "0   cv815_22456.txt    0.9931     0.104    0.736     0.160\n",
              "0   cv274_25253.txt    0.9996     0.083    0.713     0.204\n",
              "0   cv927_10681.txt    0.9210     0.069    0.852     0.079\n",
              "0   cv442_13846.txt   -0.9993     0.199    0.684     0.117\n",
              "0   cv551_10565.txt    0.9973     0.087    0.753     0.160\n",
              "..              ...       ...       ...      ...       ...\n",
              "0   cv117_24295.txt   -0.9568     0.169    0.674     0.157\n",
              "0    cv602_8300.txt   -0.9861     0.100    0.848     0.052\n",
              "0   cv560_17175.txt    0.9960     0.107    0.759     0.134\n",
              "0    cv891_6385.txt   -0.9181     0.120    0.771     0.109\n",
              "0   cv677_17715.txt    0.9880     0.101    0.772     0.127\n",
              "\n",
              "[1000 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMX-GLSTnLue"
      },
      "source": [
        "thresholds = [-0.1*x for x in range(9,0,-1)] + [0.1*x for x in range(0,10)] # creating list of threshold values between 0 to 1 in intervals of 0.05"
      ],
      "execution_count": 304,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RALF_LX0n2Bx"
      },
      "source": [
        "pos_accuracies = []\n",
        "for threshold in thresholds:\n",
        "  pos_accuracies.append(len(positive_sentiments.loc[positive_sentiments['Compound'] >= threshold])/len(positive_sentiments))"
      ],
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWOX_mzBoiaY"
      },
      "source": [
        "pos_threshold_accuracies = pd.DataFrame(pos_accuracies,thresholds)\n",
        "pos_threshold_accuracies = pd.DataFrame({'Threshold':thresholds,'Accuracy':pos_accuracies})"
      ],
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "j07bpRkyn11k",
        "outputId": "c8b729e8-e23f-42af-e8af-15e0d83a3db7"
      },
      "source": [
        "pos_threshold_accuracies"
      ],
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.9</td>\n",
              "      <td>0.884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.8</td>\n",
              "      <td>0.867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.7</td>\n",
              "      <td>0.858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.6</td>\n",
              "      <td>0.851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.4</td>\n",
              "      <td>0.840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.6</td>\n",
              "      <td>0.807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.7</td>\n",
              "      <td>0.803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.8</td>\n",
              "      <td>0.792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.9</td>\n",
              "      <td>0.749</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Threshold  Accuracy\n",
              "0        -0.9     0.884\n",
              "1        -0.8     0.867\n",
              "2        -0.7     0.858\n",
              "3        -0.6     0.851\n",
              "4        -0.5     0.843\n",
              "5        -0.4     0.840\n",
              "6        -0.3     0.834\n",
              "7        -0.2     0.831\n",
              "8        -0.1     0.831\n",
              "9         0.0     0.828\n",
              "10        0.1     0.824\n",
              "11        0.2     0.824\n",
              "12        0.3     0.822\n",
              "13        0.4     0.820\n",
              "14        0.5     0.815\n",
              "15        0.6     0.807\n",
              "16        0.7     0.803\n",
              "17        0.8     0.792\n",
              "18        0.9     0.749"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 307
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tVJ85FCtNaw"
      },
      "source": [
        "The above dataframe shows the threshold and accuracy of the positive dataset at that corresponding thresholds. The optimal threshold value can be 0.2 as there is no much increase in accuracy as we reduce the threshold down. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2gTzF_s6FTb"
      },
      "source": [
        "negative_sentiments = pd.DataFrame()"
      ],
      "execution_count": 308,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv8yW0qy6K48"
      },
      "source": [
        "for path,doc in zip(negative_file_paths,neg_text):\n",
        "  temp = sia.polarity_scores(doc)\n",
        "  negative_sentiments = negative_sentiments.append([[path,temp['compound'],temp['neg'],temp['neu'],temp['pos']]])"
      ],
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzKAVrJCBIr_"
      },
      "source": [
        "negative_sentiments.columns = ['Filename', 'Compound','Negative','Neutral','Positive']"
      ],
      "execution_count": 310,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "raVE6ycmArdy",
        "outputId": "ce2c1a70-6503-4db3-e225-0554d8e466bf"
      },
      "source": [
        "negative_sentiments"
      ],
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filename</th>\n",
              "      <th>Compound</th>\n",
              "      <th>Negative</th>\n",
              "      <th>Neutral</th>\n",
              "      <th>Positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv561_9484.txt</td>\n",
              "      <td>-0.8243</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.798</td>\n",
              "      <td>0.093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv135_12506.txt</td>\n",
              "      <td>0.9932</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.755</td>\n",
              "      <td>0.161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv568_17065.txt</td>\n",
              "      <td>-0.5106</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.772</td>\n",
              "      <td>0.112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv297_10104.txt</td>\n",
              "      <td>-0.6371</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.732</td>\n",
              "      <td>0.135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv523_18285.txt</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.833</td>\n",
              "      <td>0.100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv323_29633.txt</td>\n",
              "      <td>-0.9766</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.729</td>\n",
              "      <td>0.125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv574_23191.txt</td>\n",
              "      <td>0.9981</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.735</td>\n",
              "      <td>0.169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv385_29621.txt</td>\n",
              "      <td>0.8535</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.758</td>\n",
              "      <td>0.134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv936_17473.txt</td>\n",
              "      <td>0.9793</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.731</td>\n",
              "      <td>0.156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv396_19127.txt</td>\n",
              "      <td>0.9223</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.660</td>\n",
              "      <td>0.197</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Filename  Compound  Negative  Neutral  Positive\n",
              "0    cv561_9484.txt   -0.8243     0.109    0.798     0.093\n",
              "0   cv135_12506.txt    0.9932     0.083    0.755     0.161\n",
              "0   cv568_17065.txt   -0.5106     0.116    0.772     0.112\n",
              "0   cv297_10104.txt   -0.6371     0.134    0.732     0.135\n",
              "0   cv523_18285.txt    0.9387     0.067    0.833     0.100\n",
              "..              ...       ...       ...      ...       ...\n",
              "0   cv323_29633.txt   -0.9766     0.146    0.729     0.125\n",
              "0   cv574_23191.txt    0.9981     0.096    0.735     0.169\n",
              "0   cv385_29621.txt    0.8535     0.108    0.758     0.134\n",
              "0   cv936_17473.txt    0.9793     0.113    0.731     0.156\n",
              "0   cv396_19127.txt    0.9223     0.144    0.660     0.197\n",
              "\n",
              "[1000 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 311
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlMv3aLaArVw"
      },
      "source": [
        "neg_accuracies = []\n",
        "for threshold in thresholds:\n",
        "  neg_accuracies.append(len(negative_sentiments.loc[negative_sentiments['Compound'] >= threshold])/len(negative_sentiments))"
      ],
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNdB0Xc8boz2"
      },
      "source": [
        "neg_threshold_accuracies = pd.DataFrame(neg_accuracies,thresholds)\n",
        "neg_threshold_accuracies = pd.DataFrame({'Threshold':thresholds,'Accuracy':neg_accuracies})"
      ],
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "i9Yy-md_qLsH",
        "outputId": "a4d12041-afd4-444d-f3d6-03c83c2ee3dd"
      },
      "source": [
        "neg_threshold_accuracies"
      ],
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.9</td>\n",
              "      <td>0.684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.8</td>\n",
              "      <td>0.642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.7</td>\n",
              "      <td>0.616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.6</td>\n",
              "      <td>0.597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.4</td>\n",
              "      <td>0.579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.6</td>\n",
              "      <td>0.513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.7</td>\n",
              "      <td>0.489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.8</td>\n",
              "      <td>0.460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.9</td>\n",
              "      <td>0.412</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Threshold  Accuracy\n",
              "0        -0.9     0.684\n",
              "1        -0.8     0.642\n",
              "2        -0.7     0.616\n",
              "3        -0.6     0.597\n",
              "4        -0.5     0.583\n",
              "5        -0.4     0.579\n",
              "6        -0.3     0.564\n",
              "7        -0.2     0.561\n",
              "8        -0.1     0.556\n",
              "9         0.0     0.555\n",
              "10        0.1     0.549\n",
              "11        0.2     0.546\n",
              "12        0.3     0.539\n",
              "13        0.4     0.534\n",
              "14        0.5     0.526\n",
              "15        0.6     0.513\n",
              "16        0.7     0.489\n",
              "17        0.8     0.460\n",
              "18        0.9     0.412"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 314
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wo4QJ3RsYly"
      },
      "source": [
        "The above dataframe shows the threshold and accuracy of the negative dataset at that corresponding thresholds. The maximum threshold value to get atleast 50% of accuracy is at 0.6. The optimal threshold value can be -0.5 as we can see an elbow curve forming there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idEWKYXIAsSZ"
      },
      "source": [
        "\n",
        "**Bonus (40 points): Repeat questions 2,3 and 4 removing all stopwords. Answer the following questions: Did this change the results in any way? Why do you think so?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxd7RyuxVqEF"
      },
      "source": [
        "Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBisha1HAswj"
      },
      "source": [
        "pos_text_2 = pos_text\n",
        "neg_text_2 = neg_text"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V-tl9_BEAt-"
      },
      "source": [
        "pos_text_after_stop_words = []\n",
        "\n",
        "for text in pos_text_2:\n",
        "  temp = []\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  for token in tokens:\n",
        "    if(token not in stop_words):\n",
        "      temp.append(token)\n",
        "  pos_text_after_stop_words.append(' '.join(temp))"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaRQ8TnGUBBG"
      },
      "source": [
        "neg_text_after_stop_words = []\n",
        "\n",
        "for text in neg_text_2:\n",
        "  temp = []\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  for token in tokens:\n",
        "    if(token not in stop_words):\n",
        "      temp.append(token)\n",
        "  neg_text_after_stop_words.append(' '.join(temp))"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3wKooFlUhb0"
      },
      "source": [
        "def generate_splits_stopwords(pos_percentage, neg_percentage): # function used to split test and train data\n",
        "  pos_text_r = copy.deepcopy(pos_text_after_stop_words)\n",
        "  random.seed(123)\n",
        "  random.shuffle(pos_text_r)\n",
        "\n",
        "  neg_text_r = copy.deepcopy(neg_text_after_stop_words)\n",
        "  random.shuffle(neg_text_r)\n",
        "\n",
        "  pos_index = int((len(pos_text_r)/100)*pos_percentage)\n",
        "  neg_index = int((len(neg_text_r)/100)*neg_percentage)\n",
        "\n",
        "  return pos_text_r[:pos_index], pos_text_r[pos_index:], neg_text_r[:neg_index], neg_text_r[neg_index:]"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc_yB4ClV-T3"
      },
      "source": [
        "Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WuVxGwfUjJ7"
      },
      "source": [
        "pos_train_a, pos_test_a, neg_train_a, neg_test_a = generate_splits_stopwords(50,70)"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRoXCL1wUjFQ"
      },
      "source": [
        "train_x_a = pos_train_a + neg_train_a\n",
        "train_y_a = [1]*len(pos_train_a) + [0]*len(neg_train_a)\n",
        "\n",
        "c = list(zip(train_x_a, train_y_a))\n",
        "random.shuffle(c)\n",
        "train_x_a, train_y_a = zip(*c)"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Fc3LxE2kU8zX",
        "outputId": "58ff8cd9-dd43-4889-fec4-ac7a261d8820"
      },
      "source": [
        "model_a = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "model_a.fit(train_x_a, train_y_a)"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('multinomialnb',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc3r4cGTU_QQ"
      },
      "source": [
        "pos_train_b, pos_test_b, neg_train_b, neg_test_b = generate_splits_stopwords(70,50)"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-OHO7gsVBrS"
      },
      "source": [
        "train_x_b = pos_train_b + neg_train_b\n",
        "train_y_b = [1]*len(pos_train_b) + [0]*len(neg_train_b)\n",
        "\n",
        "c = list(zip(train_x_b, train_y_b))\n",
        "random.shuffle(c)\n",
        "train_x_b, train_y_b = zip(*c)"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8PQFDO3VVDg8",
        "outputId": "c7ad55c3-0fdc-4429-d3ff-50e57ea6f97e"
      },
      "source": [
        "model_b = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "model_b.fit(train_x_b, train_y_b)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('multinomialnb',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9VP2ESlVFA3"
      },
      "source": [
        "pos_train_c, pos_test_c, neg_train_c, neg_test_c = generate_splits_stopwords(25,25)"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM51VFj4VHIF"
      },
      "source": [
        "train_x_c = pos_train_c + neg_train_c\n",
        "train_y_c = [1]*len(pos_train_c) + [0]*len(neg_train_c)\n",
        "\n",
        "c = list(zip(train_x_c, train_y_c))\n",
        "random.shuffle(c)\n",
        "train_x_c, train_y_c = zip(*c)"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2pLbpAeOVJER",
        "outputId": "2b913f96-8267-42cd-f300-f16e1d58df9c"
      },
      "source": [
        "model_c = make_pipeline(TfidfVectorizer(), SVC())\n",
        "model_c.fit(train_x_c,train_y_c)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('svc',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOF-yF2xWB6K"
      },
      "source": [
        "Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbXOAeUOVK0T"
      },
      "source": [
        "y_test_model_a = [1]*len(pos_test_a) + [0]*len(neg_test_a)\n",
        "y_pred_model_a = model_a.predict(pos_test_a + neg_test_a)\n",
        "model_a_accuracy = sklearn.metrics.accuracy_score(y_pred_model_a,y_test_model_a)\n",
        "model_a_recall = sklearn.metrics.recall_score(y_pred_model_a,y_test_model_a)\n",
        "model_a_percision =  sklearn.metrics.precision_score(y_pred_model_a,y_test_model_a)\n",
        "model_a_f1score =  sklearn.metrics.f1_score(y_pred_model_a,y_test_model_a,average='macro')"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN0FULWwVP6Z"
      },
      "source": [
        "y_test_model_b = [1]*len(pos_test_b) + [0]*len(neg_test_b)\n",
        "y_pred_model_b = model_b.predict(pos_test_b + neg_test_b)\n",
        "model_b_accuracy = sklearn.metrics.accuracy_score(y_pred_model_b,y_test_model_b)\n",
        "model_b_recall = sklearn.metrics.recall_score(y_pred_model_b,y_test_model_b)\n",
        "model_b_percision =  sklearn.metrics.precision_score(y_pred_model_b,y_test_model_b)\n",
        "model_b_f1score =  sklearn.metrics.f1_score(y_pred_model_b,y_test_model_b,average='macro')"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xxl123sVRy3"
      },
      "source": [
        "y_test_model_c = [1]*len(pos_test_c) + [0]*len(neg_test_c)\n",
        "y_pred_model_c = model_c.predict(pos_test_c + neg_test_c)\n",
        "model_c_accuracy = sklearn.metrics.accuracy_score(y_pred_model_c,y_test_model_c)\n",
        "model_c_recall = sklearn.metrics.recall_score(y_pred_model_c,y_test_model_c)\n",
        "model_c_percision =  sklearn.metrics.precision_score(y_pred_model_c,y_test_model_c)\n",
        "model_c_f1score =  sklearn.metrics.f1_score(y_pred_model_c,y_test_model_c,average='macro')"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "XNb3b-1FVUKl",
        "outputId": "d829669f-ff8f-44ec-bdb9-b5bbfed8c662"
      },
      "source": [
        "pd.DataFrame(np.array([['Model A',50,70,model_a_accuracy,model_a_recall,model_a_percision,model_a_f1score],\n",
        "                             ['Model B',70,50,model_b_accuracy,model_b_recall,model_b_percision,model_b_f1score],\n",
        "                             ['Model C',25,25,model_c_accuracy,model_c_recall,model_c_percision,model_c_f1score],\n",
        "]),columns=['Model Name','Positive Percentage','Negative Percentage','Accuracy', 'Recall', 'Precision','F1-Score'])"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model Name</th>\n",
              "      <th>Positive Percentage</th>\n",
              "      <th>Negative Percentage</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Precision</th>\n",
              "      <th>F1-Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Model A</td>\n",
              "      <td>50</td>\n",
              "      <td>70</td>\n",
              "      <td>0.38875</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.2970085124506046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Model B</td>\n",
              "      <td>70</td>\n",
              "      <td>50</td>\n",
              "      <td>0.38125</td>\n",
              "      <td>0.37735849056603776</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.283873592838736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Model C</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.7632978723404256</td>\n",
              "      <td>0.7653333333333333</td>\n",
              "      <td>0.7639995804436986</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Model Name Positive Percentage  ...           Precision            F1-Score\n",
              "0    Model A                  50  ...               0.022  0.2970085124506046\n",
              "1    Model B                  70  ...                 1.0   0.283873592838736\n",
              "2    Model C                  25  ...  0.7653333333333333  0.7639995804436986\n",
              "\n",
              "[3 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM27vUAiWFOD"
      },
      "source": [
        "a) What is the best performing model?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DU_EtmFWUDh"
      },
      "source": [
        "Model C, which is SVM with TFIDF is the best performing model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIA7omj7WUA0"
      },
      "source": [
        "b) Why do you think this is the best performing model?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2aaCrweWT54"
      },
      "source": [
        "SVM performed best because the training set is balanced. For model 1 and model 2 the training set is highly imbalanced which might have affected model's performance. But for Model C the training set is balanced and it led to better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2TgdOoPWcjq"
      },
      "source": [
        "c) How does class imbalance play in determining polarity?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzoywgAChFUB"
      },
      "source": [
        "By looking at the above metrics table, it clearly shows that model A and model B are affected by class imbalances. For model A, the percision is 1 and recall is 0.022. This clearly shows that the model is predicting 0 for all the values. This is because the training dataset for Model A is highly unbalanced with negative labels.\n",
        "\n",
        "Similarly, for model B, the recall is 1 and precision is 0.377. As model B is inbalanced with large number of positive the model is predicting 1 for all the values.\n",
        "\n",
        "Due to the imbalance in the dataset, the models A and B have given importance to positive and negative labels respectively. Whereas Model C, being balanced dataset, performed better than the above two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSyuGxVNWg5x"
      },
      "source": [
        "d) Do you think either more data or a better model is a better approach for this kind of task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkh4F-TWWg0S"
      },
      "source": [
        "Depeding on the model, the performance of our classification task may improve. If we have large data, by using deep neural networks we might be able to build a better performing model.\n",
        "\n",
        "But, having more data doesn't always mean that the performance of the model will improve as it might lead to overfitting of the model. So we have to choose the model and dataset optimally such that it doesn't lead to overfitting or underfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwMfFRgIhQup"
      },
      "source": [
        " **Did this change the results in any way? Why do you think so?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8aSvPQihQpR"
      },
      "source": [
        "Yes, removing stopwords from the dataset has increased the performance of all the models. The accuracy, recall, precision and F1 Score have been improved for all the models because of removing stopwords.\n",
        "\n",
        "Stop words are the words which are frequently used in the any text. For example, a, is, I, the. These type of words doesn't have much significance in determining the sentiment of the text when compared to other nouns, verbs and adverbs. So, by removing stop words the models have been able to give importance to specific words which provide the sentiment of overall text better."
      ]
    }
  ]
}